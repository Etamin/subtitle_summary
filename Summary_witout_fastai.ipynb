{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dh = pd.read_json('dialogsum.train.jsonl',lines=True)\n",
    "dh = dh.loc [:,['dialogue','summary']]\n",
    "dh.columns = ['text','headlines']\n",
    "dh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console=Console(record=True)\n",
    "\n",
    "def display_df(df):\n",
    "  \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "  console=Console()\n",
    "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
    "\n",
    "  for i, row in enumerate(df.values.tolist()):\n",
    "    table.add_row(row[0], row[1])\n",
    "\n",
    "  console.print(table)\n",
    "\n",
    "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
    "                        Column(\"Steps\", justify=\"center\"),\n",
    "                        Column(\"Loss\", justify=\"center\"), \n",
    "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataSetClass(Dataset):\n",
    "  \"\"\"\n",
    "  Creating a custom dataset for reading the dataset and \n",
    "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = dataframe\n",
    "    self.source_len = source_len\n",
    "    self.summ_len = target_len\n",
    "    self.target_text = self.data[target_text]\n",
    "    self.source_text = self.data[source_text]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.target_text)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_text = str(self.source_text[index])\n",
    "    target_text = str(self.target_text[index])\n",
    "\n",
    "    #cleaning data so as to ensure data is in string type\n",
    "    source_text = ' '.join(source_text.split())\n",
    "    target_text = ' '.join(target_text.split())\n",
    "\n",
    "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "    source_ids = source['input_ids'].squeeze()\n",
    "    source_mask = source['attention_mask'].squeeze()\n",
    "    target_ids = target['input_ids'].squeeze()\n",
    "    target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids.to(dtype=torch.long), \n",
    "        'source_mask': source_mask.to(dtype=torch.long), \n",
    "        'target_ids': target_ids.to(dtype=torch.long),\n",
    "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to be called for training with the parameters passed from main function\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  model.train()\n",
    "  for _,data in enumerate(loader, 0):\n",
    "    y = data['target_ids'].to(device, dtype = torch.long)\n",
    "    y_ids = y[:, :-1].contiguous()\n",
    "    lm_labels = y[:, 1:].clone().detach()\n",
    "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "    loss = outputs[0]\n",
    "\n",
    "    if _%10==0:\n",
    "      training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "      console.print(training_logger)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
    "  \n",
    "  \"\"\"\n",
    "  T5 trainer\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # Set random seeds and deterministic pytorch for reproducibility\n",
    "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  # logging\n",
    "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "  # tokenzier for encoding the text\n",
    "  tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "  console.log(tokenizer) \n",
    "\n",
    "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "  model = model.to(device)\n",
    "  \n",
    "  # logging\n",
    "  console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "  # Importing the raw dataset\n",
    "  dataframe = dataframe[[source_text,target_text]]\n",
    "  display_df(dataframe.head(2))\n",
    "\n",
    "  \n",
    "  # Creation of Dataset and Dataloader\n",
    "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "  train_size = 0.8\n",
    "  train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "  val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "  train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "  console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "\n",
    "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "  # Defining the parameters for creation of dataloaders\n",
    "  train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "  training_loader = DataLoader(training_set, **train_params)\n",
    "  val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "  # Training loop\n",
    "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "      \n",
    "  console.log(f\"[Saving Model]...\\n\")\n",
    "  #Saving the model after training\n",
    "  path = os.path.join(output_dir, \"model_files\")\n",
    "  model.save_pretrained(path)\n",
    "  tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "  # evaluating test dataset\n",
    "  console.log(f\"[Initiating Validation]...\\n\")\n",
    "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "  \n",
    "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
    "  \n",
    "  console.log(f\"[Validation Completed.]\\n\")\n",
    "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
    "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
    "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"t5-large\",             # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\":1,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":1,          # validation batch size\n",
    "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1.5e-5,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":1024,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":100,   # max length of target text\n",
    "    \"SEED\": 42                     # set seed for reproducibility \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T5Trainer(dataframe=dh, source_text=\"text\", target_text=\"headlines\", model_params=model_params, output_dir=\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
